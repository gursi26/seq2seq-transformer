{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import re, json, pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(512, 8, 2048)\n",
    "encoder = nn.TransformerEncoder(encoder_layer, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(32, 100, 512)\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = list(text)\n",
    "    vocab = sorted(list(set(tokens)))\n",
    "    return tokens, vocab\n",
    "    \n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, txt_path, context_len):\n",
    "        self.context_len = context_len\n",
    "        self.tokenized, vocab = preprocess(\"\".join(open(txt_path).readlines()))\n",
    "        \n",
    "        self.char2idx = {\"<PAD>\": 0}\n",
    "        for i, word in enumerate(vocab):\n",
    "            self.char2idx[word] = i + 1\n",
    "        self.idx2char = {value: key for key, value in self.char2idx.items()}\n",
    "\n",
    "        self.embedded = torch.tensor([self.char2idx[c] for c in self.tokenized])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized) - (self.context_len - 1)\n",
    "    \n",
    "    def pad_and_mask(self, seq):\n",
    "        seq_len = seq.shape[0]\n",
    "        mask = torch.concat([torch.ones(seq_len), torch.zeros(self.context_len - seq_len)], dim=0).type(torch.bool)\n",
    "        seq = torch.concat([seq, torch.zeros(self.context_len - seq_len, dtype=torch.long)], dim=0)\n",
    "        return seq, mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_len = random.randint(100, self.context_len)\n",
    "        src = self.embedded[idx:idx+seq_len]\n",
    "        tgt = self.embedded[idx+seq_len]\n",
    "        src, mask = self.pad_and_mask(src)\n",
    "        return src.long(), mask, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, seq_len, device, p=0.1):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.pe = torch.arange(seq_len).unsqueeze(-1).repeat(1, d_model).type(torch.float32)\n",
    "        even_pos = torch.arange(0, d_model, 2)\n",
    "        self.pe[:, ::2] = torch.sin(self.pe[:, ::2] / (10000 ** (even_pos/d_model)))\n",
    "        self.pe[:, 1::2] = torch.cos(self.pe[:, 1::2] / (10000 ** ((even_pos + 1)/d_model)))\n",
    "        self.pe = self.pe.unsqueeze(0).to(device)\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "\n",
    "    # x has shape [batch, seq_len, embed_dim]\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert output_dim % num_heads == 0    # output_dim must be divisible by num_heads\n",
    "        self.num_heads, self.head_dim = num_heads, output_dim // num_heads\n",
    "        self.qkv_linear = nn.Linear(input_dim, output_dim * 3)\n",
    "        self.out_linear = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "    # x has shape [batch_size, seq_len, input_dim]\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        if mask is not None:\n",
    "            mask_batch_size, num_heads, d1, d2 = mask.shape\n",
    "            assert d1 == seq_len and d2 == seq_len\n",
    "            assert mask_batch_size == batch_size and num_heads == self.num_heads\n",
    "\n",
    "        # computing q, k and v across multiple heads with a single linear layer\n",
    "        qkv = self.qkv_linear(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim * 3)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        attn_output = self.scaled_dot_product(q, k, v, mask)\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).reshape(batch_size, seq_len, -1)\n",
    "        return self.out_linear(attn_output)\n",
    "\n",
    "    # q, k and v have shape [batch_size, num_heads, seq_len, head_dim]\n",
    "    def scaled_dot_product(self, q, k, v, mask):\n",
    "        d_k = k.shape[-1]\n",
    "        qk = q.matmul(k.transpose(-1, -2)) / d_k\n",
    "        if mask is not None:\n",
    "            qk = qk.masked_fill(~mask, -torch.inf)\n",
    "        attn_weights = qk.softmax(dim=-1)\n",
    "        return attn_weights.matmul(v)\n",
    "    \n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, p=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadAttention(d_model, d_model, num_heads),\n",
    "            nn.Dropout(p=p),\n",
    "            nn.LayerNorm(d_model),\n",
    "            MultiHeadAttention(d_model, d_model, num_heads),\n",
    "            nn.Dropout(p=p),\n",
    "            nn.LayerNorm(d_model),\n",
    "            self.linear_layer,\n",
    "            nn.Dropout(p=p),\n",
    "            nn.LayerNorm(d_model)\n",
    "        ])\n",
    "\n",
    "    # x has shape [batch_size, seq_len, embed_dim]\n",
    "    def forward(self, x, mask=None):\n",
    "        prev = x\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.LayerNorm):\n",
    "                x = layer(x + prev)\n",
    "                prev = x\n",
    "            elif isinstance(layer, MultiHeadAttention):\n",
    "                x = layer(x, mask)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, d_model, output_dim, num_heads, context_len, device):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Embedding(input_dim, d_model),\n",
    "            PositionalEncoder(d_model, context_len, device),\n",
    "            TransformerDecoder(d_model, self.num_heads),\n",
    "            TransformerDecoder(d_model, self.num_heads),\n",
    "            TransformerDecoder(d_model, self.num_heads),\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(d_model * context_len, output_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, TransformerDecoder):\n",
    "                x = layer(x, mask)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return self.out_proj(x.flatten(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(embedded, model, dataset, device):\n",
    "    embedded, mask = dataset.pad_and_mask(embedded)\n",
    "    embedded, mask = embedded.unsqueeze(0).to(device), mask.unsqueeze(0).to(device)\n",
    "    mask = mask.unsqueeze(1).unsqueeze(-2).repeat(1, model.num_heads, dataset.context_len, 1)\n",
    "    output = model(embedded, mask).view(-1).softmax(dim=0)\n",
    "    return torch.multinomial(output, 1)\n",
    "\n",
    "def sample(sentence, generate_len, max_seq_len, model, dataset, device):\n",
    "    tokens, _ = preprocess(sentence)\n",
    "    embedded = torch.tensor([dataset.char2idx[c] for c in tokens])\n",
    "    for _ in range(generate_len):\n",
    "        output = predict(embedded[(len(embedded) - max_seq_len):], model, dataset, device)\n",
    "        embedded = torch.concat([embedded, output.cpu()])\n",
    "    return \"\".join([dataset.idx2char[c.item()] for c in embedded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 62\n",
    "MODEL_DIM = 512\n",
    "NUM_HEADS = 8\n",
    "CONTEXT_LEN = 500\n",
    "BATCH_SIZE = 64\n",
    "SAMPLE_GENERATE_LEN = 50\n",
    "EPOCHS = 100\n",
    "\n",
    "LR = 1e-10\n",
    "BETAS = [0.9, 0.98]\n",
    "DEV = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SequenceDataset(\"/content/drive/MyDrive/transformer-files/shakespeare-sonnet.txt\", CONTEXT_LEN)\n",
    "loader = DataLoader(dataset, BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(\n",
    "    input_dim=INPUT_DIM,\n",
    "    d_model=MODEL_DIM,\n",
    "    output_dim=len(dataset.char2idx),\n",
    "    num_heads=NUM_HEADS,\n",
    "    context_len=CONTEXT_LEN,\n",
    "    device=DEV\n",
    ").to(DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=LR, betas=BETAS)\n",
    "crit = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : [0/100]:   0%|          | 3/1463 [00:30<4:05:50, 10.10s/it, loss=4.28]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m2/jysgcgj57vn69541g8m0qz_h0000gn/T/ipykernel_81531/2259180683.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(EPOCHS):\n",
    "    loop = tqdm(enumerate(loader), total=len(loader), leave=True, position=0)\n",
    "    loop.set_description(f\"Epoch : [{e}/{EPOCHS}]\")\n",
    "    total_loss = 0\n",
    "    for i, (src, mask, tgt) in loop:\n",
    "        src, mask, tgt = src.to(DEV), mask.to(DEV), tgt.to(DEV)\n",
    "        mask = mask.unsqueeze(1).unsqueeze(-2).repeat(1, model.num_heads, dataset.context_len, 1)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        yhat = model(src, mask)\n",
    "        loss = crit(yhat, tgt.view(-1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss = total_loss/(i + 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
