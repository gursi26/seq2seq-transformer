{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, txt_path, context_len):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, seq_len, device, p=0.1):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.pe = torch.arange(seq_len).unsqueeze(-1).repeat(1, d_model).type(torch.float32)\n",
    "        even_pos = torch.arange(0, d_model, 2)\n",
    "        self.pe[:, ::2] = torch.sin(self.pe[:, ::2] / (10000 ** (even_pos/d_model)))\n",
    "        self.pe[:, 1::2] = torch.cos(self.pe[:, 1::2] / (10000 ** ((even_pos + 1)/d_model)))\n",
    "        self.pe = self.pe.unsqueeze(0).to(device)\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "\n",
    "    # x has shape [batch, seq_len, embed_dim]\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert output_dim % num_heads == 0    # output_dim must be divisible by num_heads\n",
    "        self.num_heads, self.head_dim = num_heads, output_dim // num_heads\n",
    "        self.qkv_linear = nn.Linear(input_dim, output_dim * 3)\n",
    "        self.out_linear = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "    # x has shape [batch_size, seq_len, input_dim]\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        if mask is not None:\n",
    "            mask_batch_size, num_heads, d1, d2 = mask.shape\n",
    "            assert d1 == seq_len and d2 == seq_len\n",
    "            assert mask_batch_size == batch_size and num_heads == self.num_heads\n",
    "\n",
    "        # computing q, k and v across multiple heads with a single linear layer\n",
    "        qkv = self.qkv_linear(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim * 3)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        attn_output = self.scaled_dot_product(q, k, v, mask)\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).reshape(batch_size, seq_len, -1)\n",
    "        return self.out_linear(attn_output)\n",
    "\n",
    "    # q, k and v have shape [batch_size, num_heads, seq_len, head_dim]\n",
    "    def scaled_dot_product(self, q, k, v, mask):\n",
    "        d_k = k.shape[-1]\n",
    "        qk = q.matmul(k.transpose(-1, -2)) / d_k\n",
    "        if mask is not None:\n",
    "            qk = qk.masked_fill(mask, -torch.inf)\n",
    "        attn_weights = qk.softmax(dim=-1)\n",
    "        return attn_weights.matmul(v)\n",
    "    \n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, p=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadAttention(d_model, d_model, num_heads),\n",
    "            nn.Dropout(p=p),\n",
    "            nn.LayerNorm(d_model),\n",
    "            MultiHeadAttention(d_model, d_model, num_heads),\n",
    "            nn.Dropout(p=p),\n",
    "            nn.LayerNorm(d_model),\n",
    "            self.linear_layer,\n",
    "            nn.Dropout(p=p),\n",
    "            nn.LayerNorm(d_model)\n",
    "        ])\n",
    "\n",
    "    # x has shape [batch_size, seq_len, embed_dim]\n",
    "    def forward(self, x):\n",
    "        prev = x\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.LayerNorm):\n",
    "                x = layer(x + prev)\n",
    "                prev = x\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, d_model, output_dim, context_len, device):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, d_model),\n",
    "            PositionalEncoder(d_model, context_len, device),\n",
    "            TransformerDecoder(d_model, 8),\n",
    "            TransformerDecoder(d_model, 8),\n",
    "            TransformerDecoder(d_model, 8),\n",
    "        )\n",
    "        self.out_proj = nn.Linear(d_model * context_len, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return self.out_proj(x.flatten(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(\n",
    "    input_dim=300,\n",
    "    d_model=512,\n",
    "    output_dim=24558,\n",
    "    context_len=100,\n",
    "    device=\"cpu\"\n",
    ").to(\"cpu\")\n",
    "model = torch.compile(model)\n",
    "x = torch.randn(32, 100, 300).to(\"cpu\")\n",
    "# model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1270160366"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([\n",
    "    [1, 1, 0, 0],\n",
    "    [1 ,0, 0, 0],\n",
    "    [1, 1, 1, 0],\n",
    "    [1, 1, 1, 1],\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 0, 0, 0]\n",
    "])\n",
    "# [batch_size, seq_len] mask for padding tokens\n",
    "a_exp = a.unsqueeze(1).unsqueeze(-2).repeat(1, 8, 4, 1) # expanding for MHA\n",
    "a_exp[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"The-Secret-History.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.remove(\"\\n\")\n",
    "while \"\\n\" in f:\n",
    "    f.remove(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for line in f:\n",
    "    line = line.strip().lower()\n",
    "    words = set(line.split())\n",
    "    vocab = vocab.union(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for line in f:\n",
    "    line = line.strip().lower()\n",
    "    text.extend(line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204140"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 921 ms, sys: 2.07 s, total: 3 s\n",
      "Wall time: 2.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.665466666666667"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(232 * (204140 // 32) / 1000) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24558"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
